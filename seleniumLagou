import json
import math
import time
import random
import pymongo
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support.ui import WebDriverWait
from multiprocessing import Process, JoinableQueue as Queue
from selenium.webdriver.support import expected_conditions as EC

START_URL = 'https://www.lagou.com/'
LOGIN_URL = 'https://www.lagou.com/frontLogin.do'
BASE_URL = 'https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false'
WORK_INFO_URL = 'https://www.lagou.com/jobs/{}.html'
COMPANY_INFO_URL = 'https://www.lagou.com/gongsi/{}.html'
REVIEW_INFO_URL = 'https://www.lagou.com/gongsi/searchInterviewExperiences.json'

UA_LIST=[
    'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6',
    'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11',
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; Media Center PC 6.0; InfoPath.3; MS-RTC LM 8; Zune 4.7)",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; Media Center PC 6.0; InfoPath.3; MS-RTC LM 8; Zune 4.7",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; Zune 4.0; InfoPath.3; MS-RTC LM 8; .NET4.0C; .NET4.0E)",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 2.0.50727; SLCC2; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; Zune 4.0; Tablet PC 2.0; InfoPath.3; .NET4.0C; .NET4.0E)",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0"
    ]

HEADERS = {
    'Referer': 'https://www.lagou.com/jobs/list_Python?px=default&city=%E5%85%A8%E5%9B%BD',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
}

PROXY_URL = 'http://127.0.0.1:5555/random'

def generate_mongodb():
    """生成一个MongoDB对象"""
    client = pymongo.MongoClient(host='localhost', port=27017)
    db = client.lagou
    return db

def get_random_proxy(proxy_url):
    try:
        response = requests.get(proxy_url)
        print('正在获取代理')
        if response.status_code == 200:
            proxy = response.text
            print('成功获取代理%s' % proxy)
            return proxy
    except Exception as e:
        print('获取代理失败', e.args)
        return None

def get_cookies(browser):
    """动态获取cookies"""
    browser.get(START_URL)
    cookies_list = browser.get_cookies()
    cookies = {i['name']: i['value'] for i in cookies_list}
    print('获取最新cookie: %s' % cookies)
    return cookies

def generate_browser():
    """生成一个无界面谷歌浏览器驱动"""
    options = Options()
    # 看需求设置代理和无头模式(注：免费代理池的随机代理非常不稳定,经常登陆不上拉勾'
    # proxy = get_random_proxy(PROXY_URL)
    # if proxy:
    #    options.add_argument('--proxy-server=http://{}'.format(proxy))
    options.add_argument('--headless')
    browser = webdriver.Chrome(chrome_options=options)
    return browser

def login_lagou(second):
    """进入拉勾网登录就绪状态"""
    browser = generate_browser()
    wait = WebDriverWait(browser, 10)
    try:
        browser.get(LOGIN_URL)
        user = wait.until(EC.presence_of_element_located((By.XPATH, '//input[@type="text"]')))
        password = wait.until(EC.presence_of_element_located((By.XPATH, '//input[@type="password"]')))
        user.send_keys('18829040039')
        password.send_keys('xuzhihai0723')
        time.sleep(second)
        # 打开多个选项卡备用
        # browser.execute_script('window.open("https://www.lagou.com/")')
        # browser.execute_script('window.open("https://www.lagou.com/")')
        # submit = wait.until(EC.element_to_be_clickable((By.XPATH, '//input[@type="submit"]')))
        # submit.click()
        return browser
    except TimeoutException:
        # 休眠10秒,重试
        time.sleep(10)
        login_lagou(second)

def get_company_info(queue):
    # browser = generate_browser()
    browser = login_lagou(5)
    browser.set_window_rect(600, 0, 600, 800)
    db = generate_mongodb()

    while True:
        if queue.empty():
            break
        company_id = queue.get()
        try:
            browser.get(COMPANY_INFO_URL.format(company_id))
            company_info = {
                'label': browser.find_element_by_xpath('//div[@class="item_content"]/ul/li[1]/span').text,
                'inancing': browser.find_element_by_xpath('//div[@class="item_content"]/ul/li[2]/span').text,
                'scale': browser.find_element_by_xpath('//div[@class="item_content"]/ul/li[3]/span').text,
                'address': browser.find_element_by_xpath('//div[@class="item_content"]/ul/li[4]/span').text,
                'quote': browser.find_element_by_xpath('//div[@class="company_word"]').text.strip(),
                'positionNum': browser.find_element_by_xpath('//div[@class="company_data"]/ul/li[1]/strong').text.replace('"', '').strip(),
                'resume_processing_time_rate': browser.find_element_by_xpath('//div[@class="company_data"]/ul/li[2]/strong').text.replace('"', '').strip(),
                'resume_processing_duration': browser.find_element_by_xpath('//div[@class="company_data"]/ul/li[3]/strong').text.replace('"', '').strip(),
                'interview_comment_num': browser.find_element_by_xpath('//div[@class="company_data"]/ul/li[4]/strong').text.replace('"', '').strip(),
                'last_login': browser.find_element_by_xpath('//div[@class="company_data"]/ul/li[5]/strong').text.replace('"', '').strip(),
                'introduction': browser.find_element_by_xpath('//span[@class="company_content"]').text.replace('\n', ''),
            }
            name = browser.find_element_by_xpath('//div[@class="company_main"]/h1/a').text.strip()
            company_info['name'] = name
            print(company_info)
            db.company_info.insert(dict(company_info))
            print('%s公司信息已采集入库' % name)
            queue.task_done()
        except Exception as e:
            print('遇到异常', e.args, ',更换代理休眠20秒重试')
            # queue.put(company_id)
            time.sleep(10)
            browser = login_lagou(5)
            browser.set_window_rect(600, 0, 600, 800)
        time.sleep(3)
    browser.close()
    queue.join()

#def get_company_review(queue):
#    # browser = generate_browser()
#    browser = login_lagou(10)
#    browser.set_window_rect(600, 0, 600, 800)
#    db = generate_mongodb()

    # 获取cookie身份

#    cookies = get_cookies(browser)

    # 模拟随机浏览器
#    HEADERS['User-Agent'] = random.choice(UA_LIST)

#    while True:
#        if queue.empty():
#            break
#        company_id = queue.get()
#        data = {
#            'companyId': company_id,
#            'positionType': '',
#            'pageSize': '10',
#            'pageNo': '1'
#        }
#        try:
#            response = requests.post(REVIEW_INFO_URL, data=data, headers=HEADERS, cookies=cookies)

def get_work_info(queue):
    # browser = generate_browser()
    browser = login_lagou(5)
    browser.set_window_rect(100, 0, 600, 600)
    db = generate_mongodb()

    while True:
        if queue.empty():
            break
        work_id = queue.get()
        try:
            browser.get(WORK_INFO_URL.format(work_id))
            work_info = {
                'id': work_id,
                'content': browser.find_element_by_xpath('//dl[@id="job_detail"]').text.replace('\n', '')
            }
            name = browser.find_element_by_xpath('//span[@class="name"]').text
            work_info['name'] = name
            print(work_info)
            db.work_info.insert(work_info)
            print('%s职位详细信息已入库' % name)
            queue.task_done()
        except Exception as e:
            print('遇到异常', e.args, ',更换代理休眠20秒重试')
            # queue.put(work_id)
            time.sleep(10)
            browser = login_lagou(5)
            browser.set_window_rect(100, 0, 600, 600)
        time.sleep(3)
    browser.close()
    queue.join()

def get_page(headers, cookies, data, options={}):
    # 这里第一个参数需要传入字典，因为后面的option也是字典，这样才可以将第一个参数传递给option
    proxies = dict({'proxies': ''}, **options)
    response = requests.post(BASE_URL, headers=headers, cookies=cookies, data=data, proxies=proxies)
    result = json.loads(response.text)
    if result['success']:
        collection = db.works
        datas = result['content']['positionResult']['result']
        # 拉勾网最多只显示前30页，本次搜索结果最大页数向上取整。
        if math.ceil(result['content']['positionResult']['totalCount']/15) > 30:
            MAX_PAGE_NUM = 30
        else:
            MAX_PAGE_NUM = math.ceil(result['content']['positionResult']['totalCount']/15)
        print('当前职位搜索结果总页数为{}'.format(MAX_PAGE_NUM))
        print(datas)
        collection.insert(datas)
        print('页面信息已入库')
        for data in datas:
            # 将发布该条职位信息的公司ID放入companies队列中
            company_id = data['companyId']
            companies.put(company_id)
            # 将职位ID放入works队列中
            work_id = data['positionId']
            works.put(work_id)

if __name__ == '__main__':

    start = time.time()

    # 存放公司ID的队列
    companies = Queue()
    # 存放岗位信息的队列
    works = Queue()

    browser = login_lagou(5)
    browser.set_window_rect(0, 0, 200, 600)

    # 启动一个进程采集公司信息
    process_1 = Process(target=get_company_info, args=(companies,))
    process_1.start()
    print('公司信息采集进程已开启')
    time.sleep(10)

    # 启动一个进程采集岗位信息
    process_2 = Process(target=get_work_info, args=(works,))
    process_2.start()
    print('岗位信息采集进程已开启')

    # 获取一个MongoDB对象
    db = generate_mongodb()

    # 获取cookie身份
    cookies = get_cookies(browser)

    # 模拟随机浏览器
    HEADERS['User-Agent'] = random.choice(UA_LIST)

    # 最大页数
    MAX_PAGE_NUM = 30
	
    # 采集职位信息
    page = 1
    while True:
        if page > MAX_PAGE_NUM:
            print('已采集完所有页数')
            break
        data = {
            'first': 'false',
            'pn': page,
            'kd': 'python爬虫'
        }
        print('正在抓取第{}页'.format(page))
        try:
            get_page(HEADERS, cookies, data, options={})
        except Exception as e:
            print('采集遇到异常: ', e.args, ',使用代理休眠20秒重新抓取')
            proxies = {'proxies': get_random_proxy(PROXY_URL)}

            # 更新cookie身份
            cookies = get_cookies(browser)

            print('模拟随机浏览器')
            HEADERS['User-Agent'] = random.choice(UA_LIST)

            print('更换身份，正在重试')
            time.sleep(20)
            get_page(HEADERS, cookies, data, options=proxies)

        sleep_time = random.randint(1,3)
        print('第{}页采集完毕，休眠{}秒后继续抓取下一页'.format(page, sleep_time))
        time.sleep(sleep_time)
        page += 1
        # companies.put(None)
        # works.put(None)
    browser.close()

    process_1.join()
    process_2.join()

    print('耗时：', time.time() - start)
